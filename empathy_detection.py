# -*- coding: utf-8 -*-
"""Empathy_Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12T91uQ-kJ8nw1MhF7xJFfXiiAUhuFCNY

Building on the exploration and analysis done in assignment 1.
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive', force_remount= True)
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline 
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error,explained_variance_score

# Load the data - already merged data from the test group as they were  to have been empathetic 
df = pd.read_csv('/content/drive/MyDrive/Eye empathy/test_merged.csv')
quest_df = pd.read_csv('/content/drive/MyDrive/Eye empathy/Questionnaire_datasetIA.csv', encoding='latin-1')

df.head()

# Drop columns that are irrelevant or too sparse 
columns_to_drop = ['Sensor', 'Recording timestamp', 'Computer timestamp', 'Export date',
                   'Recording name', 'Recording date', 'Timeline name', 'Recording Fixation filter name', 
                   'Mouse position X', 'Mouse position Y', 'Recording software version', 'Event', 'Event value', 
                   'Project name', 'Presented Media name', 'Presented Stimulus name', 'Unnamed: 0', 'Eyetracker timestamp', 
                   'Recording start time UTC', 'Recording date UTC', 'Project name', 'Gaze point X (MCSnorm)', 
                   'Gaze point Y (MCSnorm)', 'Gaze point left X (MCSnorm)', 'Gaze point left Y (MCSnorm)', 
                   'Gaze point right X (MCSnorm)', 'Gaze point right Y (MCSnorm)', 'Fixation point X', 'Fixation point Y', 
                   'Fixation point X (MCSnorm)', 'Fixation point Y (MCSnorm)']

df.drop(columns_to_drop, axis=1, inplace=True)

# Drop rows with null values in 'Pupil diameter left' and 'Pupil diameter right' as they are the main features to be used. 
df = df.dropna(subset=['Pupil diameter left', 'Pupil diameter right'])


# Drop rows where 'Participant name' is 'Participant name'
df = df[df['Participant name'] != 'Participant name']

# Extract participant number from 'Participant name' column
df['id'] = df['Participant name'].apply(lambda x: int(x[-2:]) if isinstance(x, str) else 'NaN')
df.drop('Participant name', axis= 1, inplace = True)

df['Eye movement type'].value_counts()

# Manual label encoding of 'Eye movement type' column
eye_movement_dict = {'Fixation': 0, 'Saccade': 1, 'Unclassified': 2, 'EyesNotFound': 3}
df['Eye movement type'] = df['Eye movement type'].map(eye_movement_dict)

# Visualize sparsity of remaining features
sns.heatmap(df.isnull(), cbar=False)

#mapping empathy scores from the questionnaire dataset
score_dic = dict(zip(quest_df['Participant nr'], quest_df['Total Score original']))
df['Empathy scores'] = [score_dic.get(i) for  i in df.id]

df['Empathy scores'].value_counts()

#the DACSmm features are all in coordinate format and will be dropped

df['Eye position left Y (DACSmm)'][0:6]

df.drop(['Eye position left Y (DACSmm)', 'Eye position left Z (DACSmm)',
       'Eye position right X (DACSmm)', 'Eye position right Y (DACSmm)',
       'Eye position right Z (DACSmm)', 'Gaze point left X (DACSmm)',
       'Gaze point left Y (DACSmm)', 'Gaze point right X (DACSmm)',
       'Gaze point right Y (DACSmm)','Eye position left X (DACSmm)'], inplace = True, axis = 1)

df['Gaze direction right Z'][10]

#the gaze directions also are coordinates,

df.drop(['Gaze direction left X', 'Gaze direction left Y',
       'Gaze direction left Z', 'Gaze direction right X',
       'Gaze direction right Y', 'Gaze direction right Z'], axis = 1, inplace = True)

#dropping columns not needed or having repeated data
df.drop(['Recording start time','Recording resolution height', 'Recording resolution width',
       'Recording monitor latency','Presented Media width',
       'Presented Media height','Gaze event duration'], axis = 1, inplace = True)

df[['Validity left', 'Validity right']].describe()

df.drop(['Validity left', 'Validity right'], inplace = True,axis = 1)

df['Presented Media position X (DACSpx)'].value_counts()

#same values throughout
df.drop(['Presented Media position Y (DACSpx)','Presented Media position X (DACSpx)'], inplace = True,axis = 1)

df['Pupil diameter left']

#this feature is said to be the diameter of the pupils in millimeter. The ',' will be replaced with '.'
def rep_(num):
  num_ = float(num.replace(',', '.'))
  return num_

df['Pupil diameter left'] = df['Pupil diameter left'].apply(rep_)
df['Pupil diameter right'] = df['Pupil diameter right'].apply(rep_)

df.head()

df.dropna(inplace = True)

# Split the data into training and testing sets
X = df.drop(['Empathy scores'], axis=1)
y = df['Empathy scores']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

X_train.columns

# Train and evaluate the Stochastic Gradient Descent Regressor
# Creating a KNN model with k=3
knn = KNeighborsRegressor(n_neighbors=3)
# Train the model on the training data
knn.fit(X_train, y_train)
# Making predictions on the test data
y_pred = knn.predict(X_test)
# Evaluating the model 
print('Training Score:', knn.score(X_train,y_train))
print('MAE: ', mean_absolute_error(y_test, y_pred))
print('RMSE: ',mean_squared_error(y_test, y_pred, squared= False))
print('Explained Variance Score:',explained_variance_score(y_test, y_pred))

# Evaluating the model 
print('Training Score:', knn.score(X_train,y_train))
print('MAE: ', mean_absolute_error(y_test, y_pred))
print('RMSE: ',mean_squared_error(y_test, y_pred, squared= False))
print('Explained Variance Score:',explained_variance_score(y_test, y_pred))

#creating a scatter plot
plt.scatter(y_test,y_pred)
#line of best fit
plt.plot (np.unique (y_test), np.poly1d (np.polyfit (y_test,y_pred, 1))(np.unique (y_test)), color = 'green') 
plt.title('KNN model Prediction Accuracy')
plt.xlabel('Original Empathy Scores')
plt.ylabel('Predicted Empathy Scores')
plt.show()

# Importing necessary libraries
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score
from sklearn.preprocessing import StandardScaler

# Scaling the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Creating a Stochastic Gradient Descent Regressor
sgd = SGDRegressor()

# Training the model on the training data
sgd.fit(X_train, y_train)

# Making predictions on the test data
y_pred = sgd.predict(X_test)

# Evaluating the model
print('Training Score:', sgd.score(X_train,y_train))
print('MAE: ', mean_absolute_error(y_test, y_pred))
print('RMSE: ',mean_squared_error(y_test, y_pred, squared= False))
print('Explained Variance Score:',explained_variance_score(y_test, y_pred))

